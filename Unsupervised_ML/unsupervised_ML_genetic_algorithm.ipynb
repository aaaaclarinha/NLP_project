{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage.filters import sobel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Dataset path\n",
    "train_path = \"images/cats_dogs_light/train/*\"\n",
    "test_path = \"images/cats_dogs_light/test/*\"\n",
    "\n",
    "# Resize images to\n",
    "SIZE = 128\n",
    "\n",
    "# Function to count the number of images\n",
    "def count_images(path):\n",
    "    count = 0\n",
    "    for directory_path in glob.glob(path):\n",
    "        count += len(glob.glob(os.path.join(directory_path, \"*.jpg\")))\n",
    "    return count\n",
    "\n",
    "# Function to load images and labels\n",
    "def load_images_and_labels(path):\n",
    "    num_images = count_images(path)\n",
    "    images = np.zeros((num_images, SIZE, SIZE), dtype=np.uint8)\n",
    "    labels = np.empty(num_images, dtype=object)\n",
    "    \n",
    "    idx = 0\n",
    "    for directory_path in glob.glob(path):\n",
    "        label = os.path.basename(directory_path)\n",
    "        for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "            img = cv2.resize(img, (SIZE, SIZE))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            images[idx] = img\n",
    "            labels[idx] = label\n",
    "            idx += 1\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Load training and test datasets\n",
    "print(\"Loading training and test datasets...\")\n",
    "train_images, train_labels = load_images_and_labels(train_path)\n",
    "test_images, test_labels = load_images_and_labels(test_path)\n",
    "print(\"Datasets loaded.\")\n",
    "\n",
    "# Encode labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "train_labels_encoded = le.fit_transform(train_labels)\n",
    "test_labels_encoded = le.transform(test_labels)\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train, x_test = train_images / 255.0, test_images / 255.0\n",
    "y_train, y_test = train_labels_encoded, test_labels_encoded\n",
    "\n",
    "# Feature extractor function\n",
    "def feature_extractor(images):\n",
    "    num_images = images.shape[0]\n",
    "    image_dataset = []\n",
    "\n",
    "    def process_image(image_idx):\n",
    "        img = images[image_idx, :, :]\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        # Pixel values\n",
    "        pixel_values = img.reshape(-1)\n",
    "        df['Pixel_Value'] = pixel_values\n",
    "\n",
    "        # Gabor filters\n",
    "        num = 1\n",
    "        for theta in range(2):\n",
    "            theta = theta / 4. * np.pi\n",
    "            for sigma in (1, 3):\n",
    "                lamda = np.pi/4\n",
    "                gamma = 0.5\n",
    "                gabor_label = f'Gabor{num}'\n",
    "                kernel = cv2.getGaborKernel((9, 9), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)\n",
    "                fimg = cv2.filter2D(img, cv2.CV_8UC3, kernel).reshape(-1)\n",
    "                df[gabor_label] = fimg\n",
    "                num += 1\n",
    "\n",
    "        # Sobel filter\n",
    "        edge_sobel = sobel(img).reshape(-1)\n",
    "        df['Sobel'] = edge_sobel\n",
    "\n",
    "        return df\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_image, range(num_images)))\n",
    "\n",
    "    image_dataset = pd.concat(results, ignore_index=True)\n",
    "    return image_dataset\n",
    "\n",
    "print(\"Extracting features...\")\n",
    "train_features = feature_extractor(x_train)\n",
    "test_features = feature_extractor(x_test)\n",
    "print(\"Features extracted.\")\n",
    "\n",
    "# Reshape to a vector for Random Forest training\n",
    "X_for_RF = train_features.values.reshape((x_train.shape[0], -1))\n",
    "test_for_RF = test_features.values.reshape((x_test.shape[0], -1))\n",
    "\n",
    "# Genetic algorithm setup\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_int\", np.random.randint, 10, 400)\n",
    "toolbox.register(\"attr_max_depth\", np.random.randint, 1, 400)\n",
    "toolbox.register(\"attr_min_samples_split\", np.random.randint, 2, 100)\n",
    "toolbox.register(\"attr_min_samples_leaf\", np.random.randint, 1, 100)\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual, \n",
    "                 (toolbox.attr_int, toolbox.attr_max_depth, toolbox.attr_min_samples_split, toolbox.attr_min_samples_leaf), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "def evaluate(individual):\n",
    "    n_estimators = max(10, int(individual[0]))\n",
    "    max_depth = max(1, int(individual[1]))  # Ensure max_depth is at least 1\n",
    "    min_samples_split = max(2, int(individual[2]))  # Ensure min_samples_split is at least 2\n",
    "    min_samples_leaf = max(1, int(individual[3]))  # Ensure min_samples_leaf is at least 1\n",
    "    \n",
    "    RF_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, \n",
    "                                      min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, \n",
    "                                      random_state=42)\n",
    "    RF_model.fit(X_for_RF, y_train)\n",
    "    test_prediction = RF_model.predict(test_for_RF)\n",
    "    accuracy = metrics.accuracy_score(y_test, test_prediction)\n",
    "    \n",
    "    print(f\"Evaluating: n_estimators={n_estimators}, max_depth={max_depth}, min_samples_split={min_samples_split}, min_samples_leaf={min_samples_leaf}, accuracy={accuracy}\")\n",
    "    \n",
    "    return accuracy,\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt, low=[10, 1, 2, 1], up=[400, 400, 100, 100], indpb=0.3)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=5)  # Increased tournament size for stronger selection pressure\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "def main():\n",
    "    population = toolbox.population(n=50)  # Increased population size for more diversity\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    print(\"Starting genetic algorithm...\")\n",
    "    accuracy = 0.0\n",
    "    generation = 0\n",
    "    max_generations = 100  # Increased number of generations\n",
    "\n",
    "    # Elitism: Keep track of the best individuals\n",
    "    elite_size = 2\n",
    "\n",
    "    while accuracy < 0.95 and generation < max_generations:\n",
    "        offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.3)\n",
    "        \n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        \n",
    "        # Select the next generation population\n",
    "        population[:] = toolbox.select(offspring + population, len(population) - elite_size)\n",
    "        \n",
    "        # Add elites back to population\n",
    "        best_individuals = tools.selBest(population, elite_size)\n",
    "        population.extend(best_individuals)\n",
    "        \n",
    "        best_individual = tools.selBest(population, 1)[0]\n",
    "        accuracy = evaluate(best_individual)[0]\n",
    "        generation += 1\n",
    "        print(f\"Generation {generation}, Best individual: {best_individual}, Accuracy: {accuracy}\")\n",
    "\n",
    "    print(f\"Best individual is: {best_individual}\")\n",
    "    print(f\"Best accuracy is: {accuracy}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

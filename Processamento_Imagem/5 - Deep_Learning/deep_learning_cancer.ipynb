{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 02:41:00.137443: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-18 02:41:00.142064: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-18 02:41:00.202035: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-18 02:41:01.603692: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2997 images belonging to 3 classes.\n",
      "Found 4000 images in class lung_aca\n",
      "Found 4000 images in class lung_n\n",
      "Found 4000 images in class lung_scc\n",
      "Total images found in training directory: 12000\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbeneti/Venv/Arch_VS/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-06-18 02:41:37.680527: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/gbeneti/Venv/Arch_VS/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.6787 - loss: 1.1876\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88875, saving model to saved_models/best_model_fold.keras\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 128ms/step - accuracy: 0.6790 - loss: 1.1856 - val_accuracy: 0.8888 - val_loss: 0.3255 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8764 - loss: 0.3006\n",
      "Epoch 2: val_accuracy did not improve from 0.88875\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 158ms/step - accuracy: 0.8764 - loss: 0.3007 - val_accuracy: 0.8562 - val_loss: 0.3227 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m 97/300\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 141ms/step - accuracy: 0.8690 - loss: 0.3177"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "EPOCHS = 100\n",
    "RANDOM_SEED = 1024\n",
    "\n",
    "# Hyperparameters\n",
    "drop = 0.25\n",
    "kernel_initializer = 'he_uniform'\n",
    "optimizer = 'Adam'\n",
    "\n",
    "# Set up the directories\n",
    "train_dir = \"../Images_data/training_set\"\n",
    "test_dir = \"../Images_data/test_set\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='training_log.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Save hyperparameters\n",
    "with open('hyperparameters.txt', 'w') as f:\n",
    "    f.write(f\"Epochs: {EPOCHS}\\n\")\n",
    "    f.write(f\"Optimizer: {optimizer}\\n\")\n",
    "    f.write(f\"Kernel Initializer: {kernel_initializer}\\n\")\n",
    "    f.write(f\"Dropout: {drop}\\n\")\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Data normalization for testing\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create the test generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(32, 32),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    seed=RANDOM_SEED,\n",
    "    shuffle=False  # No shuffling to maintain order for evaluation\n",
    ")\n",
    "\n",
    "# Check the training directory structure and print the number of images found\n",
    "image_paths = []\n",
    "labels = []\n",
    "classes = sorted(os.listdir(train_dir))\n",
    "class_indices = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "for cls in classes:\n",
    "    cls_dir = os.path.join(train_dir, cls)\n",
    "    if os.path.isdir(cls_dir):\n",
    "        cls_images = os.listdir(cls_dir)\n",
    "        print(f\"Found {len(cls_images)} images in class {cls}\")\n",
    "        for img in cls_images:\n",
    "            img_path = os.path.join(cls_dir, img)\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(class_indices[cls])\n",
    "\n",
    "print(f\"Total images found in training directory: {len(image_paths)}\")\n",
    "\n",
    "# Define the model architecture\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same', input_shape=(32, 32, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(drop),\n",
    "    \n",
    "        Conv2D(64, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same'),\n",
    "        Conv2D(64, (3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(drop),\n",
    "    \n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu', kernel_initializer=kernel_initializer),\n",
    "        Dropout(drop),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    opt = Adam(use_ema=True)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < int(0.3 * EPOCHS):\n",
    "        return lr\n",
    "    else:\n",
    "        return lr # * np.exp(-1)\n",
    "\n",
    "# Function to load images and labels into numpy arrays\n",
    "def load_data(image_paths, labels, target_size):\n",
    "    data = []\n",
    "    for img_path in image_paths:\n",
    "        img = load_img(img_path, target_size=target_size)\n",
    "        img_array = img_to_array(img)\n",
    "        data.append(img_array)\n",
    "    return np.array(data), to_categorical(np.array(labels), num_classes=3)\n",
    "\n",
    "# Cross-validation settings\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Cross-validation loop\n",
    "cv_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(image_paths):\n",
    "    train_data_paths = image_paths[train_index]\n",
    "    train_labels = labels[train_index]\n",
    "    val_data_paths = image_paths[val_index]\n",
    "    val_labels = labels[val_index]\n",
    "    \n",
    "    # Load data for the current fold\n",
    "    train_data, train_labels = load_data(train_data_paths, train_labels, target_size=(32, 32))\n",
    "    val_data, val_labels = load_data(val_data_paths, val_labels, target_size=(32, 32))\n",
    "    \n",
    "    # Create ImageDataGenerators for the current fold\n",
    "    train_generator = train_datagen.flow(train_data, train_labels, batch_size=32, seed=RANDOM_SEED)\n",
    "    validation_generator = train_datagen.flow(val_data, val_labels, batch_size=32, seed=RANDOM_SEED)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = create_model()\n",
    "    \n",
    "    checkpoint = ModelCheckpoint('saved_models/best_model_fold.keras', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    log_csv = CSVLogger(f'saved_logs/my_logs_fold.csv', separator=',', append=False)\n",
    "    callbacks_list = [checkpoint, log_csv, LearningRateScheduler(scheduler)]\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks_list\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the validation data\n",
    "    _, val_acc = model.evaluate(validation_generator)\n",
    "    cv_scores.append(val_acc)\n",
    "\n",
    "# Train final model on all training data and evaluate on test data\n",
    "train_data, train_labels = load_data(image_paths, labels, target_size=(32, 32))\n",
    "train_generator = train_datagen.flow(train_data, train_labels, batch_size=32, seed=RANDOM_SEED)\n",
    "\n",
    "model = create_model()\n",
    "checkpoint = ModelCheckpoint('saved_models/best_model_final.keras', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "log_csv = CSVLogger('saved_logs/my_logs_final.csv', separator=',', append=False)\n",
    "callbacks_list = [checkpoint, log_csv, LearningRateScheduler(scheduler)]\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "_, test_acc = model.evaluate(test_generator)\n",
    "logging.info(f\"Accuracy on the test dataset: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Calculate average validation accuracy\n",
    "avg_val_acc = np.mean(cv_scores)\n",
    "logging.info(f\"Cross-Validation accuracy: {cv_scores}\")\n",
    "logging.info(f\"Average validation accuracy: {avg_val_acc * 100:.2f}%\")\n",
    "\n",
    "# Plot training history\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_validation_loss.jpg')\n",
    "    np.savetxt('training_validation_loss.txt', np.column_stack((history.history['loss'], history.history['val_loss'])), delimiter=',', header='Training Loss,Validation Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_validation_accuracy.jpg')\n",
    "    np.savetxt('training_validation_accuracy.txt', np.column_stack((history.history['accuracy'], history.history['val_accuracy'])), delimiter=',', header='Training Accuracy,Validation Accuracy')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_test = model.predict(test_generator)\n",
    "prediction_test = np.argmax(y_pred_test, axis=1)\n",
    "ground_truth = test_generator.classes\n",
    "\n",
    "# Parallelizing confusion matrix computation using ThreadPoolExecutor\n",
    "def compute_confusion_matrix(ground_truth, prediction_test):\n",
    "    return confusion_matrix(ground_truth, prediction_test)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    cm = executor.submit(compute_confusion_matrix, ground_truth, prediction_test).result()\n",
    "\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.savefig('confusion_matrix.jpg')\n",
    "np.savetxt('confusion_matrix.txt', cm, delimiter=',')\n",
    "plt.show()\n",
    "\n",
    "# Compute ROC AUC for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(ground_truth, y_pred_test[:, i], pos_label=i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Save ROC data for replots\n",
    "for i in range(3):\n",
    "    np.savetxt(f'roc_data_class_{i}.txt', np.column_stack((fpr[i], tpr[i])), delimiter=',', header='FPR,TPR')\n",
    "\n",
    "# Plot ROC AUC\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {i} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_curve.jpg')\n",
    "plt.show()\n",
    "\n",
    "logging.info(\"Finished training and evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Arch_VS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
